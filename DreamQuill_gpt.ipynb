{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO+hnJeJlpuAnpx4UeMLOYO"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a7WN2HvU9Qw8"
      },
      "outputs": [],
      "source": [
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/content/1k_stories_100_genre.csv', engine = 'python')"
      ],
      "metadata": {
        "id": "qyejVO0n93YP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['Formatted_input'] = \"[GENRE : \" + df [\"genre\"] + \"]\" + df['story']"
      ],
      "metadata": {
        "id": "utUXRZld-DrC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df[[\"genre\", \"Formatted_input\"]].head(3))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "20rYL2Ht-8RZ",
        "outputId": "4e5fb675-49c0-4936-e77c-be1fccc1dff5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "             genre                                    Formatted_input\n",
            "0  Science Fiction  [GENRE : Science Fiction]In the year 2250, Ear...\n",
            "1          Fantasy  [GENRE : Fantasy]In a land far away, where the...\n",
            "2          Mystery  [GENRE : Mystery]Once upon a time, in a small,...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ohQT6a7H_MdV",
        "outputId": "b6a233b4-07eb-4cc2-8b68-f4d903a49d06"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Data cleaning\n",
        "\n",
        "def clean(text):\n",
        "  text = str(text).lower()\n",
        "  text = re.sub(r'\\d+', '', text)\n",
        "  text = re.sub(r'[^\\w\\s]', '', text)\n",
        "  text = re.sub(r'\\s+', '', text).strip()\n",
        "  return text"
      ],
      "metadata": {
        "id": "ewzVsBnM0RQy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenization and stopwords removal\n",
        "\n",
        "stop_w = stopwords.words('english')\n",
        "\n",
        "def tokenize_and_stopwords(text):\n",
        "  tokens = word_tokenize(text)\n",
        "  f_tokens = [tokens for tokens in tokens if tokens not in stop_w and tokens.isalpha()]\n",
        "  return f_tokens\n"
      ],
      "metadata": {
        "id": "bsGOqmtq7N8k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Lemmatization\n",
        "\n",
        "Lemma = WordNetLemmatizer()\n",
        "\n",
        "def lemmatize(tokens):\n",
        "  lemma_tokens = [Lemma.lemmatize(token) for token in tokens]\n",
        "  return lemma_tokens"
      ],
      "metadata": {
        "id": "swquZtTP8Gya"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Combining all steps in one pipeline\n",
        "\n",
        "def preprocess_text(text):\n",
        "  cleaned_text = clean(text)\n",
        "  tokenized_text = tokenize_and_stopwords(cleaned_text)\n",
        "  lemmatized_text = lemmatize(tokenized_text)\n",
        "  return \" \".join(lemmatized_text)"
      ],
      "metadata": {
        "id": "GqR2mBnP8ZY8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply preprocessing to the story dataset\n",
        "\n",
        "df[\"clean_story\"] = df[\"story\"].apply(preprocess_text)"
      ],
      "metadata": {
        "id": "xUHnMgch88ab"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df[[\"genre\", \"clean_story\"]].head(5))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nN7VBSb59MV1",
        "outputId": "e46f8c2b-9ad6-4cfe-a4a2-67087b662c0a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                  genre                                        clean_story\n",
            "0       Science Fiction  intheyearearthhadmadesignificantstridesinspace...\n",
            "1               Fantasy  inalandfarawaywherethesunshonebrighterandthegr...\n",
            "2               Mystery  onceuponatimeinasmalltranquiltowncalledwhisper...\n",
            "3  Historical Adventure  onceuponatimeinthethcenturyasmallvillagenestle...\n",
            "4              Thriller  inthesundrenchedcoastalcityofstaugustineflorid...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Feature Extraction\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "texts = df[\"clean_story\"]\n",
        "vectorizer = TfidfVectorizer(max_features=5000)\n",
        "x = vectorizer.fit_transform(texts)\n",
        "\n",
        "print(\"feature matrix shape : \", x.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BZTKmez29ayh",
        "outputId": "24da90c4-d3b2-484b-8479-f52b813dccb2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "feature matrix shape :  (1000, 996)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#GPT-2\n",
        "\n",
        "'''from transformers import pipeline, set_seed\n",
        "\n",
        "generator = pipeline(\"text-generation\", model=\"gpt2\")\n",
        "set_seed(42)\n",
        "\n",
        "#Example input\n",
        "input_prompt = \"[GENRE: suspense] Her heart raced as she unfolded the letter...\"\n",
        "\n",
        "#Generate story\n",
        "story = generator(input_prompt, max_length=2500, do_sample=True, temperature=0.9)\n",
        "\n",
        "# Display result\n",
        "print(\"üìù Generated Story:\\n\", story[0][\"generated_text\"])'''"
      ],
      "metadata": {
        "id": "5tlMK620-1Ic",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "eb5675f9-1af5-4438-b60d-5be3b83a9dc1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'from transformers import pipeline, set_seed\\n\\ngenerator = pipeline(\"text-generation\", model=\"gpt2\")\\nset_seed(42)\\n\\n#Example input\\ninput_prompt = \"[GENRE: suspense] Her heart raced as she unfolded the letter...\"\\n\\n#Generate story\\nstory = generator(input_prompt, max_length=2500, do_sample=True, temperature=0.9)\\n\\n# Display result\\nprint(\"üìù Generated Story:\\n\", story[0][\"generated_text\"])'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Gemini\n",
        "\n",
        "'''import google.generativeai as genai\n",
        "\n",
        "genai.configure(api_key = \"AIzaSyAcof4s82txyxLuFkF8N5K5WGivxPK6sqM\")\n",
        "\n",
        "#Load the Model\n",
        "Model = genai.GenerativeModel(\"gemini-1.5-pro\")\n",
        "\n",
        "# Content generation\n",
        "\n",
        "Output = Model.generate_content(\"[GENRE: Thriller] A knock echoed at midnight...\")\n",
        "print(Output.text)'''"
      ],
      "metadata": {
        "id": "Xi8NSFN-bi0_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "d1c380b5-73d4-4e3c-953c-081ee87e74ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'import google.generativeai as genai\\n\\ngenai.configure(api_key = \"AIzaSyAcof4s82txyxLuFkF8N5K5WGivxPK6sqM\")\\n\\n#Load the Model\\nModel = genai.GenerativeModel(\"gemini-1.5-pro\")\\n\\n# Content generation  \\n\\nOutput = Model.generate_content(\"[GENRE: Thriller] A knock echoed at midnight...\")\\nprint(Output.text)'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "309ae1b2"
      },
      "source": [
        "#!pip install anthropic"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Claude 4 sonnet\n",
        "\n",
        "'''import anthropic\n",
        "\n",
        "# üîë Set your API key\n",
        "client = anthropic.Anthropic(api_key=\"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\")\n",
        "\n",
        "# üîπ Build your prompt from extracted features\n",
        "prompt = \"I want to write a romantic thriller about Henry and Lisa, enemies who fall in love. Add suspense and give it a happy ending.\"\n",
        "\n",
        "# üîπ Send prompt to Claude\n",
        "response = client.messages.create(\n",
        "    model=\"claude-4-sonnet-20240620\",\n",
        "    max_tokens=3000,\n",
        "    temperature=0.9,\n",
        "    system=\"You are a genre-aware storytelling assistant.\",\n",
        "    messages=[\n",
        "        {\"role\": \"user\", \"content\": prompt}\n",
        "    ]\n",
        ")\n",
        "\n",
        "# üîπ Display the generated story\n",
        "print(response.content[0].text)'''\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "id": "7gKie_o3IqhP",
        "outputId": "e11dcb15-b6c1-447a-c29c-6ee8ecfe8b0e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'import anthropic\\n\\n# üîë Set your API key\\nclient = anthropic.Anthropic(api_key=\"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\")\\n\\n# üîπ Build your prompt from extracted features\\nprompt = \"I want to write a romantic thriller about Henry and Lisa, enemies who fall in love. Add suspense and give it a happy ending.\"\\n\\n# üîπ Send prompt to Claude\\nresponse = client.messages.create(\\n    model=\"claude-4-sonnet-20240620\",\\n    max_tokens=3000,\\n    temperature=0.9,\\n    system=\"You are a genre-aware storytelling assistant.\",\\n    messages=[\\n        {\"role\": \"user\", \"content\": prompt}\\n    ]\\n)\\n\\n# üîπ Display the generated story\\nprint(response.content[0].text)'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# GPT-4\n",
        "\n",
        "import openai\n",
        "import pandas as pd\n",
        "\n",
        "# üîë Set your OpenAI API key\n",
        "openai.api_key = \"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\"  # Replace with your actual key\n",
        "\n",
        "# üîπ Load your prompt dataset (already cleaned and feature-extracted)\n",
        "df = pd.read_csv(\"/content/1k_stories_100_genre.csv\")  # Update path if needed\n",
        "\n",
        "# üîπ Prepare a list to store outputs\n",
        "generated_stories = []\n",
        "\n",
        "# üîÅ Loop through prompts to generate stories\n",
        "for idx, row in df.iterrows():\n",
        "    prompt = row[\"formatted_input\"]  # Column should include genre-tagged prompt\n",
        "\n",
        "    # Call GPT-4 with structured prompt\n",
        "    response = openai.ChatCompletion.create(\n",
        "        model=\"gpt-4\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"You are a genre-aware storytelling assistant.\"},\n",
        "            {\"role\": \"user\", \"content\": prompt}\n",
        "        ],\n",
        "        max_tokens=3000,  # ~2500+ words depending on complexity\n",
        "        temperature=0.9,\n",
        "        top_p=1.0,\n",
        "        frequency_penalty=0.2,\n",
        "        presence_penalty=0.3\n",
        "    )\n",
        "\n",
        "    story = response['choices'][0]['message']['content']\n",
        "\n",
        "    generated_stories.append({\n",
        "        \"genre\": row[\"genre\"],\n",
        "        \"prompt\": prompt,\n",
        "        \"story\": story\n",
        "    })\n",
        "\n",
        "# üîπ Convert to DataFrame\n",
        "gen_df = pd.DataFrame(generated_stories)\n",
        "\n",
        "# üîπ Save your generated anthology\n",
        "gen_df.to_csv(\"/content/dreamquill_generated_stories.csv\", index=False)\n",
        "\n",
        "# üîπ Preview the first few stories\n",
        "print(gen_df.head(3))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3gCOZifGItu-",
        "outputId": "1fbc8902-bbb0-4c17-c38d-220e009d435c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "'formatted_input'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3804\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3805\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3806\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mindex.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mindex.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'formatted_input'",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-27-4118378272.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m# üîÅ Loop through prompts to generate stories\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterrows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mprompt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"formatted_input\"\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# Column should include genre-tagged prompt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;31m# Call GPT-4 with structured prompt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1120\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mkey_is_scalar\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1121\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m         \u001b[0;31m# Convert generator to list before going through hashable part\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m_get_value\u001b[0;34m(self, label, takeable)\u001b[0m\n\u001b[1;32m   1235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1236\u001b[0m         \u001b[0;31m# Similar to Index.get_value, but we do not fall back to positional\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1237\u001b[0;31m         \u001b[0mloc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1238\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1239\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3810\u001b[0m             ):\n\u001b[1;32m   3811\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mInvalidIndexError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3812\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3813\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3814\u001b[0m             \u001b[0;31m# If we have a listlike key, _check_indexing_error will raise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'formatted_input'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KRXpl3jeKmLw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}